{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "playground for testing evaluation metrics on consitency with other author (Ben Hammer)\n",
    "and plausibility checks\n",
    "'''\n",
    "import numpy as np\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "As a benchmark create case of a dumb model without any predictive power and test, how this model performs\n",
    "measured in MAP@5 metric\n",
    "Test set contains 15,610 images. \n",
    "\n",
    "compute:\n",
    "1) a \"dumb\" model, that maps the images to the individuals uniform randomly, \n",
    "2) a model, that maps the images to the individuals along a probability distribution \n",
    "   that reflects the distribution of frequencies of observed whales (whale #1, \n",
    "   with 34 images, will occure 34 times more likely than whale # 2.500 with one image)\n",
    "3) like 2), but without the category \"new whale\" (~800 occurences) \n",
    "'''\n",
    "from scipy.stats import rv_discrete\n",
    "\n",
    "def Compute_MAP(probs = 'uniform'):\n",
    "\n",
    "    train_list = read_csv(file_name = \"data/train.csv\")   # for testing whole train data set\n",
    "\n",
    "    whales, counts = get_whales(train_list)\n",
    "    # print(\"{} individuals\".format(len(counts)))  \n",
    "\n",
    "    # to each image in train_list map a ranked list of max_pred whales\n",
    "    # as random number between 1 and # of individuals in scenario (indeces in whale list)\n",
    "    max_pred = 5\n",
    "    dummy_preds = []\n",
    "    \n",
    "    if probs == 'uniform':\n",
    "        for i in range(len(train_list)):\n",
    "            ranks = np.random.randint(0,len(counts),max_pred)\n",
    "            dummy_preds.append(ranks)\n",
    "\n",
    "    elif probs == 'weighted':\n",
    "        x = np.arange(0,len(whales))\n",
    "        total_whales = np.sum(counts)\n",
    "        # probability distribution = normalised distribution of frequencies\n",
    "        px=[whales[i][1]/total_whales for i in range(len(whales))]   \n",
    "        # create ranked list of max_pred = 5 labels per image in train_list\n",
    "        for i in range(len(train_list)):\n",
    "            # sample labels accodring probability distribution defined above\n",
    "            ranks=rv_discrete(values=(x,px)).rvs(size=max_pred)  \n",
    "            dummy_preds.append(ranks)\n",
    "\n",
    "    elif probs == 'weighted_without_new':\n",
    "        x = np.arange(1,len(whales))\n",
    "        total_whales = np.sum(counts[1:])\n",
    "        # probability distribution = normalised distribution of frequencies\n",
    "        px=[whales[i][1]/total_whales for i in range(1,len(whales))]\n",
    "        # create ranked list of max_pred = 5 labels per image in train_list\n",
    "        for i in range(len(train_list)):\n",
    "            # sample labels accodring probability distribution defined above\n",
    "            ranks=rv_discrete(values=(x,px)).rvs(size=max_pred)  \n",
    "            dummy_preds.append(ranks)\n",
    "            \n",
    "    else:\n",
    "        raise AssertionError\n",
    "\n",
    "\n",
    "    # get list of true labels: retrieve whale number from name\n",
    "    true_labels = []\n",
    "    for i, img in enumerate(train_list):\n",
    "        name = img[1]\n",
    "        true_labels.append([i for i, whale in enumerate(whales) if whale[0] == name][0])\n",
    "\n",
    "    return mean_average_precision(dummy_preds, true_labels, max_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MAP with uniform probability distribution over whales:  0.000548223350254\n",
      "\n",
      " MAP with prob. distribution over whales as in training data:  0.0154230118443\n",
      "\n",
      " MAP with prob. distribution over whales as in training data, without 'new whale':  0.00119627749577\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n MAP with uniform probability distribution over whales: \", \n",
    "      Compute_MAP(probs = 'uniform'))\n",
    "print(\"\\n MAP with prob. distribution over whales as in training data: \", \n",
    "      Compute_MAP(probs = 'weighted'))\n",
    "print(\"\\n MAP with prob. distribution over whales as in training data, without 'new whale': \",\\\n",
    "      Compute_MAP(probs = 'weighted_without_new'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from \n",
    "# https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "# only to double check validity of MAP metric functions (see below)\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "    # print(\"ben Hammer predicted\", predicted)\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:  \n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "            # print(\"ben Hammer add score\", score)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score # / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision my take:  0.5\n",
      "average precision Ben Hammer:  0.5\n",
      "mean average precision my take:  0.416666666667\n",
      "mean average precision Ben Hammer:  0.416666666667\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "evaluation metrics MAP@5\n",
    "sources: \n",
    "https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "https://www.kaggle.com/c/FacebookRecruiting/discussion/2002\n",
    "https://en.wikipedia.org/wiki/Information_retrieval\n",
    "Note, that the metric is designed for \"document retrieval\", \n",
    "where many outcomes might be true (= \"relevant documents\")\n",
    "Our case is specific, as there is only one \"relevant document\" per prediction \n",
    "(= the true prediction)\n",
    "'''\n",
    "\n",
    "# test implementation against Ben Hammers one\n",
    "test = average_precision([1,12,3,12,8],12,5)\n",
    "print(\"average precision my take: \",test)\n",
    "\n",
    "test = apk([12],[1,12,3,12,8],5)\n",
    "print(\"average precision Ben Hammer: \",test)\n",
    "\n",
    "\n",
    "test = mean_average_precision([[1,13,3,12,8], [5,3,12,3,6], [8,6,11,2,4]],[12,15,8],5 )\n",
    "print(\"mean average precision my take: \",test)\n",
    "\n",
    "test = mapk([[12],[15],[8]], [[1,13,3,12,8], [5,3,12,3,6], [8,6,11,8,4]], 5)\n",
    "print(\"mean average precision Ben Hammer: \",test)\n",
    "\n",
    "true_lables = [12,15,8]\n",
    "model_predictions = [[1,13,3,12,8], [5,3,12,3,6], [8,6,11,8,4]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
