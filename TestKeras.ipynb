{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/.local/share/virtualenvs/Projekt-JHRjFyBJ/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "120  images copied as training data\n",
      "56  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n"
     ]
    }
   ],
   "source": [
    "# global variables\n",
    "all_train_dir = \"data/train\"     # directory with original kaggle training data\n",
    "all_train_csv = \"data/train.csv\" # original kaggle train.csv file\n",
    "train_dir = \"data/model_train\"\n",
    "train_csv = \"data/model_train.csv\"\n",
    "valid_dir = \"data/model_valid\"\n",
    "valid_csv = \"data/model_valid.csv\"\n",
    "\n",
    "num_classes = 7     # number of whales to be considered (in order of occuurence)\n",
    "max_preds = 5       # number of ranked predictions (default 5)\n",
    "batch_size = 16     # used for training as well as validation\n",
    "train_valid = 0.7   # ratio training / validation data\n",
    "\n",
    "# create training environment for training data\n",
    "num_train_imgs, num_valid_imgs = create_small_case(\n",
    "       sel_whales = np.arange(1,num_classes+1),  # whales to be considered\n",
    "       all_train_dir = all_train_dir,\n",
    "       all_train_csv = all_train_csv,\n",
    "       train_dir = train_dir,\n",
    "       train_csv = train_csv,\n",
    "       valid_dir = valid_dir,\n",
    "       valid_csv = valid_csv,\n",
    "       train_valid = train_valid,\n",
    "       sub_dirs = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)                     # new added as https://towardsdatascience.com/transfer-learning-using-keras-d804b2e04ef8\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "# and a logistic layer\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "# metrics='accuracy' causes the model to store and report accuracy (train and validate)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 images belonging to 7 classes.\n",
      "Found 56 images belonging to 7 classes.\n",
      "{'w_7554f44': 3, 'w_693c9ee': 2, 'w_1287fbc': 0, 'w_fd1cb9d': 6, 'w_1eafe46': 1, 'w_98baff9': 4, 'w_ab4cae2': 5}\n",
      "{0: 'w_1287fbc', 1: 'w_1eafe46', 2: 'w_693c9ee', 3: 'w_7554f44', 4: 'w_98baff9', 5: 'w_ab4cae2', 6: 'w_fd1cb9d'}\n"
     ]
    }
   ],
   "source": [
    "# define image generator\n",
    "train_gen = image.ImageDataGenerator(\n",
    "    # featurewise_center=True,\n",
    "    # featurewise_std_normalization=True,\n",
    "    rescale = 1./255,   # redundant with featurewise_center ? \n",
    "    # preprocessing_function=preprocess_input, not used in most examples\n",
    "    # horizontal_flip = True,    # no, as individual shapes are looked for\n",
    "    fill_mode = \"nearest\",\n",
    "    zoom_range = 0.3,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range=0.3,\n",
    "    rotation_range=30)\n",
    "\n",
    "train_flow = train_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    # save_to_dir = \"data/model_train/augmented\",    \n",
    "    # color_mode = \"grayscale\",\n",
    "    target_size = (299,299),\n",
    "    batch_size = batch_size, \n",
    "    class_mode = \"categorical\")\n",
    "\n",
    "valid_gen = image.ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    fill_mode = \"nearest\")\n",
    "\n",
    "valid_flow = valid_gen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size = (299,299),\n",
    "    batch_size = batch_size,        \n",
    "    class_mode = \"categorical\")\n",
    "\n",
    "whale_class_map = (train_flow.class_indices)           # get dict mapping whalenames --> class_no\n",
    "class_whale_map = make_label_dict(directory=train_dir) # get dict mapping class_no --> whalenames\n",
    "print(whale_class_map)\n",
    "print(class_whale_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'InceptV3_1_Epochs_7_classes_2nd.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fcd16e822f32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# reload model that gets not stuck on training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'InceptV3_1_Epochs_7_classes_2nd.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/Whales-4YGaJ5HX/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;31m# instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Whales-4YGaJ5HX/lib/python3.5/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Whales-4YGaJ5HX/lib/python3.5/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'InceptV3_1_Epochs_7_classes_2nd.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# reload model that gets not stuck on training\n",
    "# from keras.models import load_model\n",
    "# model = load_model('InceptV3_1_Epochs_7_classes_2nd.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(\n",
    "    train_flow, \n",
    "    steps_per_epoch = num_train_imgs//batch_size,\n",
    "    verbose = 2, \n",
    "    validation_data = valid_flow,   \n",
    "    validation_steps = num_valid_imgs//batch_size,\n",
    "    epochs=50)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 143s - loss: 1.9422 - acc: 0.2491 - val_loss: 1.9831 - val_acc: 0.1282\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 137s - loss: 1.8522 - acc: 0.2364 - val_loss: 1.8058 - val_acc: 0.4103\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 136s - loss: 1.6444 - acc: 0.3543 - val_loss: 1.8145 - val_acc: 0.2564\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 159s - loss: 1.8091 - acc: 0.3017 - val_loss: 2.0996 - val_acc: 0.1538\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 163s - loss: 1.7562 - acc: 0.4141 - val_loss: 1.7109 - val_acc: 0.4103\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 156s - loss: 1.7219 - acc: 0.3572 - val_loss: 1.9924 - val_acc: 0.2051\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 156s - loss: 1.5179 - acc: 0.4462 - val_loss: 1.6511 - val_acc: 0.3846\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 161s - loss: 1.6906 - acc: 0.3548 - val_loss: 1.6114 - val_acc: 0.4615\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 161s - loss: 1.5394 - acc: 0.4487 - val_loss: 1.4971 - val_acc: 0.4615\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5b02b1038e73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_flow\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# to be used later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_valid_imgs\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         epochs=1)              \n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'180129_InceptV3_20_Epochs_7_classes_CV.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2193\u001b[0m                                 \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m                                 \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m                                 workers=0)\n\u001b[0m\u001b[1;32m   2196\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m                             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2324\u001b[0m                                      \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m                                      str(generator_output))\n\u001b[0;32m-> 2326\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1889\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train in cross validation loop\n",
    "for i in range(20):\n",
    "    # create new environment with new random train / valid split\n",
    "    num_train_imgs, num_valid_imgs = create_small_case(\n",
    "       sel_whales = np.arange(1,num_classes+1),  # whales to be considered\n",
    "       all_train_dir = all_train_dir,\n",
    "       all_train_csv = all_train_csv,\n",
    "       train_dir = train_dir,\n",
    "       train_csv = train_csv,\n",
    "       valid_dir = valid_dir,\n",
    "       valid_csv = valid_csv,\n",
    "       train_valid = 0.7,\n",
    "       sub_dirs = True) \n",
    "\n",
    "    # here change hyperparameters.........\n",
    "    \n",
    "    # define image generator\n",
    "    train_gen = image.ImageDataGenerator(\n",
    "        # featurewise_center=True,\n",
    "        # featurewise_std_normalization=True,\n",
    "        rescale = 1./255,   # redundant with featurewise_center ? \n",
    "        # preprocessing_function=preprocess_input, not used in most examples\n",
    "        # horizontal_flip = True,    # no, as individual shapes are looked for\n",
    "        fill_mode = \"nearest\",\n",
    "        zoom_range = 0.3,\n",
    "        width_shift_range = 0.3,\n",
    "        height_shift_range=0.3,\n",
    "        rotation_range=30)\n",
    "\n",
    "    train_flow = train_gen.flow_from_directory(\n",
    "        train_dir,\n",
    "        # save_to_dir = \"data/model_train/augmented\",    \n",
    "        # color_mode = \"grayscale\",\n",
    "        target_size = (299,299),\n",
    "        batch_size = batch_size, \n",
    "        class_mode = \"categorical\")\n",
    "\n",
    "    valid_gen = image.ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        fill_mode = \"nearest\")\n",
    "\n",
    "    valid_flow = valid_gen.flow_from_directory(\n",
    "        valid_dir,\n",
    "        target_size = (299,299),\n",
    "        batch_size = batch_size,         \n",
    "        class_mode = \"categorical\") \n",
    "\n",
    "    hist = model.fit_generator(\n",
    "        train_flow, \n",
    "        steps_per_epoch = num_train_imgs//batch_size,\n",
    "        verbose = 2, \n",
    "        validation_data = valid_flow,   # to be used later\n",
    "        validation_steps = num_valid_imgs//batch_size,\n",
    "        epochs=10)              \n",
    "\n",
    "model.save('180129_InceptV3_20_Epochs_7_classes_CV_gaga.h5')\n",
    "\n",
    "# here the model started already from pretrained status (initial loss was ~8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old directory removed data/model_test\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_test  validation:  None\n",
      "176  images copied as training data\n",
      "0  images copied as validation data\n",
      "write csv file with training data: data/model_test.csv\n"
     ]
    }
   ],
   "source": [
    "# training seems so work - accuracy of ~0.5 on training and validation data\n",
    "\n",
    "# try to verify on test data --> no success so far\n",
    "\n",
    "# use all training data of the first num_classes (7) whales a test data.\n",
    "# no good practice, but all training data have been augmented, so at least some indication\n",
    "# about predictive power of model\n",
    "test_dir = \"data/model_test\"\n",
    "test_csv = \"data/model_test.csv\"\n",
    "num_train_imgs, num_valid_imgs = create_small_case(\n",
    "       sel_whales = np.arange(1,num_classes+1),  # whales to be considered\n",
    "       all_train_dir = all_train_dir,\n",
    "       all_train_csv = all_train_csv,\n",
    "       train_dir = test_dir,\n",
    "       train_csv = test_csv,\n",
    "       valid_dir = None,     # no validation, copy all data into test_dir \"data/model_test\"\n",
    "       valid_csv = None,\n",
    "       train_valid = 1.,\n",
    "       sub_dirs = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 176 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# for test Purposes !!!\n",
    "\n",
    "# valid_gen = image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_gen = image.ImageDataGenerator(\n",
    "    # featurewise_center=True,\n",
    "    # featurewise_std_normalization=True,\n",
    "    rescale = 1./255,\n",
    "    # preprocessing_function=preprocess_input,   # model specific function\n",
    "    fill_mode = \"nearest\")\n",
    "\n",
    "test_flow = test_gen.flow_from_directory(\n",
    "    test_dir,\n",
    "    # color_mode = \"grayscale\",\n",
    "    batch_size = batch_size,     \n",
    "    target_size = (299,299),\n",
    "    class_mode = \"categorical\")    # use \"None\" ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 191s 17s/step\n",
      "(176, 7)\n",
      "[[  4.63210978e-03   3.00654769e-03   2.08380647e-04   3.28651840e-05\n",
      "    2.13375757e-03   2.42068578e-04   9.89744365e-01]\n",
      " [  4.22893502e-02   1.91685081e-01   1.02025822e-01   1.22443028e-02\n",
      "    2.54318833e-01   3.79027516e-01   1.84090622e-02]\n",
      " [  4.87957336e-02   3.56731176e-01   1.06019750e-01   2.23210137e-02\n",
      "    1.65488258e-01   2.62976795e-01   3.76672558e-02]\n",
      " [  1.11918822e-01   3.65958400e-02   1.60670914e-02   8.33930506e-04\n",
      "    6.80789471e-01   1.00539647e-01   5.32551333e-02]\n",
      " [  1.64128207e-02   3.37458588e-02   4.00195569e-02   4.13647760e-03\n",
      "    7.77367234e-01   1.02771342e-01   2.55466010e-02]\n",
      " [  6.20104000e-03   8.65606666e-01   3.90951335e-03   4.19794087e-04\n",
      "    9.49497707e-03   6.82832021e-03   1.07539691e-01]\n",
      " [  1.24325147e-02   1.17013408e-02   1.69343483e-02   9.31790099e-04\n",
      "    8.58521998e-01   9.30807367e-02   6.39727805e-03]\n",
      " [  2.70989574e-02   5.69240987e-01   7.86153376e-02   7.00570410e-03\n",
      "    1.14280954e-01   1.72118634e-01   3.16394903e-02]\n",
      " [  5.97229116e-02   3.86115313e-02   2.65900511e-02   3.94767337e-03\n",
      "    3.93650353e-01   2.77946275e-02   4.49682921e-01]\n",
      " [  1.75727624e-02   4.44667876e-01   3.92937250e-02   1.31709839e-03\n",
      "    3.76334459e-01   1.05533734e-01   1.52802188e-02]]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict_generator(test_flow, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w_1eafe46': 1, 'w_7554f44': 3, 'w_fd1cb9d': 6, 'w_ab4cae2': 5, 'w_98baff9': 4, 'w_1287fbc': 0, 'w_693c9ee': 2}\n",
      "{0: 'w_1287fbc', 1: 'w_1eafe46', 2: 'w_693c9ee', 3: 'w_7554f44', 4: 'w_98baff9', 5: 'w_ab4cae2', 6: 'w_fd1cb9d'}\n"
     ]
    }
   ],
   "source": [
    "whale_class_map = (test_flow.class_indices)           # get dict mapping whalenames --> class_no\n",
    "class_whale_map = make_label_dict(directory=test_dir) # get dict mapping class_no --> whalenames\n",
    "print(whale_class_map)\n",
    "print(class_whale_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 7)\n",
      "[[  4.63210978e-03   3.00654769e-03   2.08380647e-04   3.28651840e-05\n",
      "    2.13375757e-03   2.42068578e-04   9.89744365e-01]\n",
      " [  4.22893502e-02   1.91685081e-01   1.02025822e-01   1.22443028e-02\n",
      "    2.54318833e-01   3.79027516e-01   1.84090622e-02]\n",
      " [  4.87957336e-02   3.56731176e-01   1.06019750e-01   2.23210137e-02\n",
      "    1.65488258e-01   2.62976795e-01   3.76672558e-02]\n",
      " [  1.11918822e-01   3.65958400e-02   1.60670914e-02   8.33930506e-04\n",
      "    6.80789471e-01   1.00539647e-01   5.32551333e-02]\n",
      " [  1.64128207e-02   3.37458588e-02   4.00195569e-02   4.13647760e-03\n",
      "    7.77367234e-01   1.02771342e-01   2.55466010e-02]\n",
      " [  6.20104000e-03   8.65606666e-01   3.90951335e-03   4.19794087e-04\n",
      "    9.49497707e-03   6.82832021e-03   1.07539691e-01]\n",
      " [  1.24325147e-02   1.17013408e-02   1.69343483e-02   9.31790099e-04\n",
      "    8.58521998e-01   9.30807367e-02   6.39727805e-03]\n",
      " [  2.70989574e-02   5.69240987e-01   7.86153376e-02   7.00570410e-03\n",
      "    1.14280954e-01   1.72118634e-01   3.16394903e-02]\n",
      " [  5.97229116e-02   3.86115313e-02   2.65900511e-02   3.94767337e-03\n",
      "    3.93650353e-01   2.77946275e-02   4.49682921e-01]\n",
      " [  1.75727624e-02   4.44667876e-01   3.92937250e-02   1.31709839e-03\n",
      "    3.76334459e-01   1.05533734e-01   1.52802188e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(preds.shape)\n",
    "print(preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predictions: \n",
      " [['w_fd1cb9d' 'w_1287fbc' 'w_1eafe46' 'w_98baff9' 'w_ab4cae2']\n",
      " ['w_ab4cae2' 'w_98baff9' 'w_1eafe46' 'w_693c9ee' 'w_1287fbc']\n",
      " ['w_1eafe46' 'w_ab4cae2' 'w_98baff9' 'w_693c9ee' 'w_1287fbc']\n",
      " ['w_98baff9' 'w_1287fbc' 'w_ab4cae2' 'w_fd1cb9d' 'w_1eafe46']\n",
      " ['w_98baff9' 'w_ab4cae2' 'w_693c9ee' 'w_1eafe46' 'w_fd1cb9d']\n",
      " ['w_1eafe46' 'w_fd1cb9d' 'w_98baff9' 'w_ab4cae2' 'w_1287fbc']\n",
      " ['w_98baff9' 'w_ab4cae2' 'w_693c9ee' 'w_1287fbc' 'w_1eafe46']\n",
      " ['w_1eafe46' 'w_ab4cae2' 'w_98baff9' 'w_693c9ee' 'w_fd1cb9d']\n",
      " ['w_fd1cb9d' 'w_98baff9' 'w_1287fbc' 'w_1eafe46' 'w_ab4cae2']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_ab4cae2' 'w_693c9ee' 'w_1287fbc']\n",
      " ['w_98baff9' 'w_ab4cae2' 'w_1eafe46' 'w_fd1cb9d' 'w_1287fbc']\n",
      " ['w_ab4cae2' 'w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_1287fbc']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_ab4cae2' 'w_1287fbc']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_ab4cae2' 'w_fd1cb9d' 'w_693c9ee']\n",
      " ['w_7554f44' 'w_ab4cae2' 'w_1eafe46' 'w_693c9ee' 'w_98baff9']\n",
      " ['w_1eafe46' 'w_ab4cae2' 'w_98baff9' 'w_693c9ee' 'w_fd1cb9d']\n",
      " ['w_fd1cb9d' 'w_1eafe46' 'w_1287fbc' 'w_98baff9' 'w_693c9ee']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_ab4cae2' 'w_693c9ee' 'w_1287fbc']\n",
      " ['w_ab4cae2' 'w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_7554f44']\n",
      " ['w_98baff9' 'w_ab4cae2' 'w_1eafe46' 'w_693c9ee' 'w_1287fbc']]\n",
      "true labels \n",
      " ['w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc'\n",
      " 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc'\n",
      " 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc'\n",
      " 'w_1287fbc' 'w_1287fbc']\n"
     ]
    }
   ],
   "source": [
    "# ge list of model predictions: one ordered list of maxpred whalenames per image\n",
    "top_k = preds.argsort()[:, -max_preds:][:, ::-1]\n",
    "# top_k = preds.argsort()[:, -max_preds:]\n",
    "model_preds = [([class_whale_map[i] for i in line]) for line in top_k]  \n",
    "\n",
    "# get list of true labels: one whalename per image\n",
    "# test_list = read_csv(file_name = test_csv)    # list with (filename, whalename)\n",
    "true_labels = []\n",
    "for fn in test_flow.filenames:\n",
    "    offset, filename = fn.split('/')\n",
    "    whale = [line[1] for line in test_list if line[0]==filename][0]\n",
    "    true_labels.append(whale)\n",
    "\n",
    "print(\"model predictions: \\n\", np.array(model_preds)[0:20])\n",
    "print(\"true labels \\n\", np.array(true_labels)[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP 0.321022727273\n",
      "Dummy MAP weighted 0.251325757576\n",
      "Dummy MAP weighted 0.311931818182\n",
      "Dummy MAP weighted 0.283049242424\n",
      "Dummy MAP weighted 0.324621212121\n",
      "Dummy MAP weighted 0.264962121212\n",
      "Dummy MAP weighted 0.273863636364\n",
      "Dummy MAP weighted 0.310700757576\n",
      "Dummy MAP weighted 0.261363636364\n",
      "Dummy MAP weighted 0.317140151515\n",
      "Dummy MAP weighted 0.289583333333\n"
     ]
    }
   ],
   "source": [
    "MAP = mean_average_precision(model_preds, true_labels, max_preds)\n",
    "print(\"MAP\", MAP)\n",
    "\n",
    "for i in range(10):\n",
    "    Dummy_map = Dummy_MAP(probs = 'weighted', distributed_as = train_csv, image_no = len(test_list))\n",
    "    print(\"Dummy MAP weighted\", Dummy_map)\n",
    "\n",
    "# MAP only slightly higher than averag dummy MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
